# -*- coding: utf-8 -*-
"""AI_In_Health_Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dq8GAqUcF6g8EbhbfHlubSa85Vfcu1uX

# Fine tuning a BioBERT model to identify medical entities from a corpus of Clinical Notes using MedSpacy
"""

import pandas as pd
import ast
from datasets import Dataset
from transformers import AutoTokenizer
from collections import defaultdict
import pandas as pd
import medspacy
from medspacy.ner import TargetRule
from medspacy.visualization import visualize_ent
import re
import torch
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments
from transformers import Trainer
from transformers import DataCollatorForTokenClassification
import evaluate

# load the NOTEEVENTS table
notes = pd.read_csv('NOTEEVENTS.csv', usecols=['TEXT', 'CATEGORY'])

# filter only Discharge Summaries or Nursing Notes
notes = notes[notes['CATEGORY'].isin(['Discharge summary', 'Nursing'])].dropna().head(1000)

# clean text
notes['TEXT'] = notes['TEXT'].str.replace('\n', ' ').str.strip()

#
notes.to_csv('Filtered_Noteevents_.csv', index=False)

# load the data from note events
notes = pd.read_csv('/content/Filtered_Noteevents_.csv')

# Load the MIMIC discharge summary data
df = notes.dropna(subset=["TEXT"])

# Initialize MedSpaCy pipeline
nlp = medspacy.load()
target_matcher = nlp.get_pipe("medspacy_target_matcher")

rules = [
    # PROBLEMS
    TargetRule("pneumonia", "PROBLEM"),
    TargetRule("heart failure", "PROBLEM"),
    TargetRule("diabetes mellitus", "PROBLEM"),
    TargetRule("atrial fibrillation", "PROBLEM"),
    TargetRule("sepsis", "PROBLEM"),
    TargetRule("stroke", "PROBLEM"),
    TargetRule("coronary artery disease", "PROBLEM"),
    TargetRule("hypertension", "PROBLEM"),
    TargetRule("hyperlipidemia", "PROBLEM"),
    TargetRule("renal failure", "PROBLEM"),
    TargetRule("copd", "PROBLEM"),
    TargetRule("infection", "PROBLEM"),
    TargetRule("cancer", "PROBLEM"),

    # MEDICATIONS
    TargetRule("metformin", "MEDICATION"),
    TargetRule("insulin", "MEDICATION"),
    TargetRule("warfarin", "MEDICATION"),
    TargetRule("lisinopril", "MEDICATION"),
    TargetRule("lasix", "MEDICATION"),
    TargetRule("heparin", "MEDICATION"),
    TargetRule("vancomycin", "MEDICATION"),
    TargetRule("aspirin", "MEDICATION"),
    TargetRule("statin", "MEDICATION"),
    TargetRule("amiodarone", "MEDICATION"),
    TargetRule("coumadin", "MEDICATION"),

    # TESTS
    TargetRule("ecg", "TEST"),
    TargetRule("ct scan", "TEST"),
    TargetRule("mri", "TEST"),
    TargetRule("chest x-ray", "TEST"),
    TargetRule("blood culture", "TEST"),
    TargetRule("echocardiogram", "TEST"),
    TargetRule("cbc", "TEST"),
    TargetRule("urinalysis", "TEST"),

    # PROCEDURES
    TargetRule("intubation", "PROCEDURE"),
    TargetRule("dialysis", "PROCEDURE"),
    TargetRule("biopsy", "PROCEDURE"),
    TargetRule("surgery", "PROCEDURE"),
    TargetRule("catheterization", "PROCEDURE"),
    TargetRule("thoracentesis", "PROCEDURE"),
    TargetRule("transfusion", "PROCEDURE"),
    TargetRule("colonoscopy", "PROCEDURE"),
    TargetRule("bronchoscopy", "PROCEDURE"),
    TargetRule("tracheostomy", "PROCEDURE"),
]

# Add rules to matcher
target_matcher.add(rules)

# Define concept extraction function
def extract_targets(text):
    doc = nlp(text)
    return [{"text": ent.text, "label": ent.label_} for ent in doc.ents]

# Apply to DataFrame
df["target_extraction"] = df["TEXT"].apply(extract_targets)

# Show some output
print(df[["TEXT", "target_extraction"]].head(5).to_string())

# Visualize targets
doc = nlp(df['TEXT'][3])
visualize_ent(doc)

# save CSV for future use
df.to_csv('targets.csv', index=False)

# Load data
df = pd.read_csv("targets.csv")
df["target_extraction"] = df["target_extraction"].apply(ast.literal_eval)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1")

# Define labels
label_list = ["O", "B-PROBLEM", "I-PROBLEM", "B-MEDICATION", "I-MEDICATION",
              "B-TEST", "I-TEST", "B-PROCEDURE", "I-PROCEDURE"]
label_to_id = {label: i for i, label in enumerate(label_list)}
id_to_label = {i: label for label, i in label_to_id.items()}

def align_tokens_and_labels(text, entities):
    tokens = tokenizer.tokenize(text)
    labels = ["O"] * len(tokens)

    for entity in entities:
        ent_text = entity["text"]
        ent_label = entity["label"]

        # Find all case-insensitive matches
        for match in re.finditer(re.escape(ent_text), text, flags=re.IGNORECASE):
            start, end = match.start(), match.end()
            word_tokens = tokenizer.tokenize(text[start:end])
            if not word_tokens:
                continue
            for idx, token in enumerate(word_tokens):
                try:
                    # Find the index of the token within the tokens list, starting the search from the beginning
                    token_index = tokens.index(token)
                    if idx == 0:
                        labels[token_index] = f"B-{ent_label.upper()}"
                    else:
                        labels[token_index] = f"I-{ent_label.upper()}"
                except ValueError:
                    continue
    return tokens, labels

# Prepare dataset
all_tokens, all_labels = [], []
for i in range(len(df)):
    tokens, labels = align_tokens_and_labels(df.loc[i, "TEXT"], df.loc[i, "target_extraction"])
    if tokens and labels:
        all_tokens.append(tokens)
        all_labels.append(labels)

# Build HuggingFace Dataset
hf_dataset = Dataset.from_dict({"tokens": all_tokens, "ner_tags": all_labels})

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True, max_length=512)
    labels = []

    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id.get(label[word_idx], 0))
            else:
                # For subword tokens, use I-label if not already O
                if label[word_idx].startswith("B-"):
                    label_ids.append(label_to_id[label[word_idx].replace("B-", "I-")])
                else:
                    label_ids.append(label_to_id.get(label[word_idx], 0))
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForTokenClassification.from_pretrained(
    "dmis-lab/biobert-base-cased-v1.1",
    num_labels=len(label_list)
).to(device)

# Make sure your tokenizer is initialized already
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)

training_args = TrainingArguments(
    output_dir="./ner_output",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.model.to(device)

trainer.train()

seqeval = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.evaluate()

trainer.save_model("biobert-clinical-ner")
tokenizer.save_pretrained("biobert-clinical-ner")
